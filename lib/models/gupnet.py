import torch
import torch.nn as nn
import numpy as np
import logging

from lib.backbones.resnet import resnet50, resnet18
from lib.backbones.dla import dla34, dla102, dla60, dla169
from lib.backbones.dlaup import DLAUp
from lib.backbones.dlaup import DLAUpv2

import torchvision.ops.roi_align as roi_align
from lib.losses.loss_function import extract_input_from_tensor
from lib.helpers.decode_helper import _topk,_nms
from lib.projective.projective_utils import *
from lib.lie_deformable_lib.lie_deformable_conv import LieDeformableConv2D, adapt_model, reset_adapt_model
from lib.depth_reciprocal_lib.depth_reciprocal import DepthReciprocalConv2d
from lib.helpers.homography_helper import get_intrinsics_from_fov
from lib.helpers.rays import cameras_to_rays, trans_to_crop_params
from lib.helpers.rays_face import plucker_embedding
from lib.helpers.ground_plane import ground_depth

def weights_init_xavier(m):
    classname = m.__class__.__name__
    if classname.find('Linear') != -1:
        nn.init.xavier_uniform_(m.weight)
        if m.bias is not None:
            nn.init.constant_(m.bias, 0.0)
    elif classname.find('Conv') != -1:
        nn.init.xavier_uniform_(m.weight)
        if m.bias is not None:
            nn.init.constant_(m.bias, 0.0)
    elif classname.find('BatchNorm') != -1:
        if m.affine:
            nn.init.constant_(m.weight, 1.0)
            nn.init.constant_(m.bias, 0.0)
 
def weights_init_classifier(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.xavier_uniform_(m.weight)
        if m.bias is not None:
            nn.init.constant_(m.bias, 0.0)
    elif classname.find('Linear') != -1:
        nn.init.normal_(m.weight, std=0.001)
        try:
            if m.bias:
                nn.init.constant_(m.bias, 0.0)
        except:
            nn.init.constant_(m.bias, 0.0)

def postprocess_depth(depth, depth_pp, version=2):
    EPS = 1e-2
    depth_out = depth
    if version == 1:
        depth_out = depth - depth_pp
    elif version == 2:
        depth_out = depth / (1 + 0.13 * depth_pp)
    elif version == 3:
        depth_out = depth - 2.67 * depth_pp + 11.92 * depth_pp/(depth + EPS)

    return depth_out

class GUPNet(nn.Module):
    def __init__(self, backbone='dla34', neck='DLAUp', downsample=4, mean_size=None, cfg= None):
        assert downsample in [4, 8, 16, 32]
        super().__init__()


        self.cfg = cfg['model']
        self.cfg_parent = cfg
        self.num_classes = len(self.cfg_parent['dataset']['writelist'])
        self.get_backbone_features = False if 'get_backbone_features'  not in self.cfg_parent['tester'].keys() else self.cfg_parent['tester']['get_backbone_features']
        self.backbone = globals()[backbone](pretrained=True, return_levels=True)
        self.head_conv = 256  # default setting for head conv
        self.mean_size = nn.Parameter(torch.tensor(mean_size,dtype=torch.float32),requires_grad=False)
        self.cls_num = mean_size.shape[0]
        channels = self.backbone.channels  # channels list for feature maps generated by backbone
        self.first_level = int(np.log2(downsample))
        scales = [2 ** i for i in range(len(channels[self.first_level:]))]
        self.feat_up = globals()[neck](channels[self.first_level:], scales_list=scales)

        self.use_conv             = "vanilla"                     if 'use_conv'             not in self.cfg.keys() else self.cfg['use_conv']
        self.sesn_scales          = [0.83, 1.0, 1.2]              if 'sesn_scales'          not in self.cfg.keys() else self.cfg['sesn_scales']
        self.replace_all          = True                          if 'replace_all'          not in self.cfg.keys() else self.cfg['replace_all']
        self.replace_layer_names  = None                          if 'replace_layer_names'  not in self.cfg.keys() else self.cfg['replace_layer_names']
        self.replace_style        = "max_scale_after_dla34_layer" if 'replace_style'        not in self.cfg.keys() else self.cfg['replace_style']
        self.scale_index_for_init = 0                             if 'scale_index_for_init' not in self.cfg.keys() else self.cfg['scale_index_for_init']
        self.sesn_padding_mode    = "constant"                    if 'sesn_padding_mode'    not in self.cfg.keys() else self.cfg['sesn_padding_mode']

        self.gd_homo                 = torch.tensor([[1, 0, 0], [0, 1., 0], [0, 0, 1.]]) if cfg is None or 'gd_homo' not in self.cfg.keys() else torch.tensor(self.cfg['gd_homo'])
        self.replace                 = ['first'] if cfg is None or 'replace'                not in self.cfg.keys() else self.cfg['replace']
        self.init_offset             = 0.1       if cfg is None or 'init_offset'             not in self.cfg.keys() else self.cfg['init_offset']
        self.coord_conv              = True      if cfg is None or 'coord_conv'              not in self.cfg.keys() else self.cfg['coord_conv']
        self.offset_append_coord     = False     if cfg is None or 'offset_append_coord'     not in self.cfg.keys() else self.cfg['offset_append_coord']
        self.explicit_convolve_coord = False     if cfg is None or 'explicit_convolve_coord' not in self.cfg.keys() else self.cfg['explicit_convolve_coord']
        self.first_coord_conv        = False     if cfg is None or 'first_coord_conv'        not in self.cfg.keys() else self.cfg['first_coord_conv']
        self.use_normal              = False     if cfg is None or 'use_normal'              not in self.cfg.keys() else self.cfg['use_normal']
        self.coord_conv_style        = "vanilla" if cfg is None or 'coord_conv_style'        not in self.cfg.keys() else self.cfg['coord_conv_style']
        self.coord_conv_merging      = "append"  if cfg is None or 'coord_conv_merging'      not in self.cfg.keys() else self.cfg['coord_conv_merging']
        self.coord_fourier_scale     = 10.0      if cfg is None or 'coord_fourier_scale'     not in self.cfg.keys() else self.cfg['coord_fourier_scale']
        self.coord_fourier_channels  = 8         if cfg is None or 'coord_fourier_channels'  not in self.cfg.keys() else self.cfg['coord_fourier_channels']
        self.depth_pp                = 0.0       if cfg is None or 'depth_pp'                not in self.cfg.keys() else self.cfg['depth_pp']
        self.depth_pp_version        = 1         if cfg is None or 'depth_pp_version'        not in self.cfg.keys() else self.cfg['depth_pp_version']
        self.lie_pos                 = 'first'   if cfg is None or 'lie_pos'                 not in self.cfg.keys() else self.cfg['lie_pos']
        self.use_crop_for_plucker    = False     if cfg is None or 'use_crop_for_plucker'    not in self.cfg.keys() else self.cfg['use_crop_for_plucker']
        self.plucker_version         = 1         if cfg is None or 'plucker_version'         not in self.cfg.keys() else self.cfg['plucker_version']
        self.canny_dilation          = 0         if cfg is None or 'canny_dilation'          not in self.cfg.keys() else self.cfg['canny_dilation']
        self.use_ground_plane        = False     if cfg is None or 'use_ground_plane'        not in self.cfg.keys() else self.cfg['use_ground_plane']
        self.ground_formulation      = 'depth'   if cfg is None or 'ground_formulation'      not in self.cfg.keys() else self.cfg['ground_formulation']
        self.ground_alpha            = 'product' if cfg is None or 'ground_alpha'            not in self.cfg.keys() else self.cfg['ground_alpha']
        self.ground_alpha_max        = -1        if cfg is None or 'ground_alpha_max'        not in self.cfg.keys() else self.cfg['ground_alpha_max']
        self.ground_depth_regress    = False     if cfg is None or 'ground_depth_regress'    not in self.cfg.keys() else self.cfg['ground_depth_regress']
        self.ground_geo_depth_twice  = False     if cfg is None or 'ground_geo_depth_twice'  not in self.cfg.keys() else self.cfg['ground_geo_depth_twice']
        self.ground_geo_depth_half   = False     if cfg is None or 'ground_geo_depth_half'   not in self.cfg.keys() else self.cfg['ground_geo_depth_half']
        self.ground_merging_learnt    = False     if cfg is None or 'ground_merging_learnt'    not in self.cfg.keys() else self.cfg['ground_merging_learnt']
        self.ground_depth_positive   = False     if cfg is None or 'ground_depth_positive'   not in self.cfg.keys() else self.cfg['ground_depth_positive']
        if self.use_ground_plane:
            logging.info("Using ground plane with formulation= {} alpha= {} alpha_max= {} regress= {} twice= {} half= {} ground_depth_positive= {} learnt= {}...".format(self.ground_formulation, self.ground_alpha, self.ground_alpha_max, self.ground_depth_regress, self.ground_geo_depth_twice, self.ground_geo_depth_half, self.ground_depth_positive, self.ground_merging_learnt))

        self.depth_reciprocal_pos   = 'last'       if cfg is None or 'depth_reciprocal_pos'   not in self.cfg.keys() else self.cfg['depth_reciprocal_pos']
        self.depth_reciprocal_style = "deformable" if cfg is None or 'depth_reciprocal_style' not in self.cfg.keys() else self.cfg['depth_reciprocal_style']
        self.depth_reciprocal_warp  = True         if cfg is None or 'depth_reciprocal_warp'  not in self.cfg.keys() else self.cfg['depth_reciprocal_warp']
        self.depth_reciprocal_Dmin  = 1            if cfg is None or 'depth_reciprocal_Dmin'  not in self.cfg.keys() else self.cfg['depth_reciprocal_Dmin']
        self.depth_reciprocal_Dmax  = 50           if cfg is None or 'depth_reciprocal_Dmax'  not in self.cfg.keys() else self.cfg['depth_reciprocal_Dmax']
        self.depth_reciprocal_focal = 307.98       if cfg is None or 'depth_reciprocal_focal' not in self.cfg.keys() else self.cfg['depth_reciprocal_focal']
        self.depth_reciprocal_formulation = "depth"if cfg is None or 'depth_reciprocal_formulation' not in self.cfg.keys() else self.cfg['depth_reciprocal_formulation']
        self.depth_geometry_method  = "max"        if cfg is None or 'depth_geometry_method'  not in self.cfg.keys() else self.cfg['depth_geometry_method']
        self.depth_project_operator = "max"        if cfg is None or 'depth_project_operator' not in self.cfg.keys() else self.cfg['depth_project_operator']
        self.depth_full_homo        = "False"      if cfg is None or 'depth_full_homo'        not in self.cfg.keys() else self.cfg['depth_full_homo']

        if self.first_coord_conv:
            logging.info("Using first layer with coordinate convolutions...")
        if self.use_conv in ["lie", "deformable", "motion_basis"]:
            logging.info("Initializing offset layers with {:.2f}".format(self.init_offset))


        if self.use_conv == "sesn":
            output_string = "Replace by {} conv... scales= {} | padding= {} | scale_index_for_init= {}".format(\
                            self.use_conv, ",".join([str(t) for t in self.sesn_scales]), self.sesn_padding_mode, \
                            self.scale_index_for_init)
            logging.info(output_string)
            self.backbone = replace_and_initialize_with_transform_weights(base= self.backbone, first_child_name= None,\
                                                                          sesn_scales= self.sesn_scales,\
                                                                          replace_all= self.replace_all,\
                                                                          replace_layer_names= self.replace_layer_names,\
                                                                          replace_style= self.replace_style,\
                                                                          scale_index_for_init= self.scale_index_for_init,\
                                                                          sesn_padding_mode= self.sesn_padding_mode)
        elif self.use_conv == "dilated":
            output_string   = "Replace by {} conv... ".format(self.use_conv)
            logging.info(output_string)
            add_dilated_conv(network= self.backbone, scales= self.sesn_scales, debug= False)
            logging.info(self.backbone)

        elif self.use_conv == "depth_reciprocal":
            output_string = "Replace {} by {} conv... ".format(self.depth_reciprocal_pos, self.use_conv)
            logging.info(output_string)

            if backbone == "dla34":
                if "level3" in self.depth_reciprocal_pos or 'all' in self.depth_reciprocal_pos:
                    self.backbone.level3.tree2.tree1.conv1 = DepthReciprocalConv2d(conv= self.backbone.level3.tree2.tree1.conv1,
                                                                          pass_warped_images= self.depth_reciprocal_warp,
                                                                          style= self.depth_reciprocal_style,
                                                                          Dmin= self.depth_reciprocal_Dmin,
                                                                          Dmax= self.depth_reciprocal_Dmax,
                                                                          focal= self.depth_reciprocal_focal,
                                                                          formulation= self.depth_reciprocal_formulation,
                                                                          geometry_method= self.depth_geometry_method,
                                                                          project_operator= self.depth_project_operator,
                                                                          full_homo= self.depth_full_homo,
                                                                          )
                    self.backbone.level3.tree2.tree1.conv2 = DepthReciprocalConv2d(conv= self.backbone.level3.tree2.tree1.conv2,
                                                                          pass_warped_images= self.depth_reciprocal_warp,
                                                                          style= self.depth_reciprocal_style,
                                                                          Dmin= self.depth_reciprocal_Dmin,
                                                                          Dmax= self.depth_reciprocal_Dmax,
                                                                          focal= self.depth_reciprocal_focal,
                                                                          formulation= self.depth_reciprocal_formulation,
                                                                          geometry_method= self.depth_geometry_method,
                                                                          project_operator= self.depth_project_operator,
                                                                          full_homo= self.depth_full_homo,
                                                                          )
                    self.backbone.level3.tree2.tree2.conv1 = DepthReciprocalConv2d(conv= self.backbone.level3.tree2.tree2.conv1,
                                                                          pass_warped_images= self.depth_reciprocal_warp,
                                                                          style= self.depth_reciprocal_style,
                                                                          Dmin= self.depth_reciprocal_Dmin,
                                                                          Dmax= self.depth_reciprocal_Dmax,
                                                                          focal= self.depth_reciprocal_focal,
                                                                          formulation= self.depth_reciprocal_formulation,
                                                                          geometry_method= self.depth_geometry_method,
                                                                          project_operator= self.depth_project_operator,
                                                                          full_homo= self.depth_full_homo,
                                                                          )
                    self.backbone.level3.tree2.tree2.conv2 = DepthReciprocalConv2d(conv= self.backbone.level3.tree2.tree2.conv2,
                                                                          pass_warped_images= self.depth_reciprocal_warp,
                                                                          style= self.depth_reciprocal_style,
                                                                          Dmin= self.depth_reciprocal_Dmin,
                                                                          Dmax= self.depth_reciprocal_Dmax,
                                                                          focal= self.depth_reciprocal_focal,
                                                                          formulation= self.depth_reciprocal_formulation,
                                                                          geometry_method= self.depth_geometry_method,
                                                                          project_operator= self.depth_project_operator,
                                                                          full_homo= self.depth_full_homo,
                                                                          )
                    self.backbone.level3.tree2.root.conv = DepthReciprocalConv2d(conv= self.backbone.level3.tree2.root.conv,
                                                                          pass_warped_images= self.depth_reciprocal_warp,
                                                                          style= self.depth_reciprocal_style,
                                                                          Dmin= self.depth_reciprocal_Dmin,
                                                                          Dmax= self.depth_reciprocal_Dmax,
                                                                          focal= self.depth_reciprocal_focal,
                                                                          formulation= self.depth_reciprocal_formulation,
                                                                          geometry_method= self.depth_geometry_method,
                                                                          project_operator= self.depth_project_operator,
                                                                          full_homo= self.depth_full_homo,
                                                                          )

                if "level4" in self.depth_reciprocal_pos or 'all' in self.depth_reciprocal_pos:
                    self.backbone.level4.tree2.tree1.conv1 = DepthReciprocalConv2d(conv= self.backbone.level4.tree2.tree1.conv1,
                                                                          pass_warped_images= self.depth_reciprocal_warp,
                                                                          style= self.depth_reciprocal_style,
                                                                          Dmin= self.depth_reciprocal_Dmin,
                                                                          Dmax= self.depth_reciprocal_Dmax,
                                                                          focal= self.depth_reciprocal_focal,
                                                                          formulation= self.depth_reciprocal_formulation,
                                                                          geometry_method= self.depth_geometry_method,
                                                                          project_operator= self.depth_project_operator,
                                                                          full_homo= self.depth_full_homo,
                                                                          )
                    self.backbone.level4.tree2.tree1.conv2 = DepthReciprocalConv2d(conv= self.backbone.level4.tree2.tree1.conv2,
                                                                          pass_warped_images= self.depth_reciprocal_warp,
                                                                          style= self.depth_reciprocal_style,
                                                                          Dmin= self.depth_reciprocal_Dmin,
                                                                          Dmax= self.depth_reciprocal_Dmax,
                                                                          focal= self.depth_reciprocal_focal,
                                                                          formulation= self.depth_reciprocal_formulation,
                                                                          geometry_method= self.depth_geometry_method,
                                                                          project_operator= self.depth_project_operator,
                                                                          full_homo= self.depth_full_homo,
                                                                          )
                    self.backbone.level4.tree2.tree2.conv1 = DepthReciprocalConv2d(conv= self.backbone.level4.tree2.tree2.conv1,
                                                                          pass_warped_images= self.depth_reciprocal_warp,
                                                                          style= self.depth_reciprocal_style,
                                                                          Dmin= self.depth_reciprocal_Dmin,
                                                                          Dmax= self.depth_reciprocal_Dmax,
                                                                          focal= self.depth_reciprocal_focal,
                                                                          formulation= self.depth_reciprocal_formulation,
                                                                          geometry_method= self.depth_geometry_method,
                                                                          project_operator= self.depth_project_operator,
                                                                          full_homo= self.depth_full_homo,
                                                                          )
                    self.backbone.level4.tree2.tree2.conv2 = DepthReciprocalConv2d(conv= self.backbone.level4.tree2.tree2.conv2,
                                                                          pass_warped_images= self.depth_reciprocal_warp,
                                                                          style= self.depth_reciprocal_style,
                                                                          Dmin= self.depth_reciprocal_Dmin,
                                                                          Dmax= self.depth_reciprocal_Dmax,
                                                                          focal= self.depth_reciprocal_focal,
                                                                          formulation= self.depth_reciprocal_formulation,
                                                                          geometry_method= self.depth_geometry_method,
                                                                          project_operator= self.depth_project_operator,
                                                                          full_homo= self.depth_full_homo,
                                                                          )
                    self.backbone.level4.tree2.root.conv = DepthReciprocalConv2d(conv= self.backbone.level4.tree2.root.conv,
                                                                          pass_warped_images= self.depth_reciprocal_warp,
                                                                          style= self.depth_reciprocal_style,
                                                                          Dmin= self.depth_reciprocal_Dmin,
                                                                          Dmax= self.depth_reciprocal_Dmax,
                                                                          focal= self.depth_reciprocal_focal,
                                                                          formulation= self.depth_reciprocal_formulation,
                                                                          geometry_method= self.depth_geometry_method,
                                                                          project_operator= self.depth_project_operator,
                                                                          full_homo= self.depth_full_homo,
                                                                          )

                if "level5" in self.depth_reciprocal_pos or 'all' in self.depth_reciprocal_pos:
                    self.backbone.level5.tree1.conv1 = DepthReciprocalConv2d(conv= self.backbone.level5.tree1.conv1,
                                                                          pass_warped_images= self.depth_reciprocal_warp,
                                                                          style= self.depth_reciprocal_style,
                                                                          Dmin= self.depth_reciprocal_Dmin,
                                                                          Dmax= self.depth_reciprocal_Dmax,
                                                                          focal= self.depth_reciprocal_focal,
                                                                          formulation= self.depth_reciprocal_formulation,
                                                                          geometry_method= self.depth_geometry_method,
                                                                          project_operator= self.depth_project_operator,
                                                                          full_homo= self.depth_full_homo,
                                                                          )
                    self.backbone.level5.tree1.conv2 = DepthReciprocalConv2d(conv= self.backbone.level5.tree1.conv2,
                                                                          pass_warped_images= self.depth_reciprocal_warp,
                                                                          style= self.depth_reciprocal_style,
                                                                          Dmin= self.depth_reciprocal_Dmin,
                                                                          Dmax= self.depth_reciprocal_Dmax,
                                                                          focal= self.depth_reciprocal_focal,
                                                                          formulation= self.depth_reciprocal_formulation,
                                                                          geometry_method= self.depth_geometry_method,
                                                                          project_operator= self.depth_project_operator,
                                                                          full_homo= self.depth_full_homo,
                                                                          )
                    self.backbone.level5.tree2.conv1 = DepthReciprocalConv2d(conv= self.backbone.level5.tree2.conv1,
                                                                          pass_warped_images= self.depth_reciprocal_warp,
                                                                          style= self.depth_reciprocal_style,
                                                                          Dmin= self.depth_reciprocal_Dmin,
                                                                          Dmax= self.depth_reciprocal_Dmax,
                                                                          focal= self.depth_reciprocal_focal,
                                                                          formulation= self.depth_reciprocal_formulation,
                                                                          geometry_method= self.depth_geometry_method,
                                                                          project_operator= self.depth_project_operator,
                                                                          full_homo= self.depth_full_homo,
                                                                          )
                    self.backbone.level5.tree2.conv2 = DepthReciprocalConv2d(conv= self.backbone.level5.tree2.conv2,
                                                                          pass_warped_images= self.depth_reciprocal_warp,
                                                                          style= self.depth_reciprocal_style,
                                                                          Dmin= self.depth_reciprocal_Dmin,
                                                                          Dmax= self.depth_reciprocal_Dmax,
                                                                          focal= self.depth_reciprocal_focal,
                                                                          formulation= self.depth_reciprocal_formulation,
                                                                          geometry_method= self.depth_geometry_method,
                                                                          project_operator= self.depth_project_operator,
                                                                          full_homo= self.depth_full_homo,
                                                                          )
                    self.backbone.level5.root.conv = DepthReciprocalConv2d(conv= self.backbone.level5.root.conv,
                                                                          pass_warped_images= self.depth_reciprocal_warp,
                                                                          style= self.depth_reciprocal_style,
                                                                          Dmin= self.depth_reciprocal_Dmin,
                                                                          Dmax= self.depth_reciprocal_Dmax,
                                                                          focal= self.depth_reciprocal_focal,
                                                                          formulation= self.depth_reciprocal_formulation,
                                                                          geometry_method= self.depth_geometry_method,
                                                                          project_operator= self.depth_project_operator,
                                                                          full_homo= self.depth_full_homo,
                                                                          )
            elif backbone == "resnet18":
                if "last" in self.depth_reciprocal_pos or "layer4" in self.depth_reciprocal_pos or 'all' in self.depth_reciprocal_pos:
                    # All layer4 replaced
                    self.backbone.layer4[1].conv1 = DepthReciprocalConv2d(conv= self.backbone.layer4[1].conv1,
                                                                          pass_warped_images= self.depth_reciprocal_warp,
                                                                          style= self.depth_reciprocal_style,
                                                                          Dmin= self.depth_reciprocal_Dmin,
                                                                          Dmax= self.depth_reciprocal_Dmax,
                                                                          focal= self.depth_reciprocal_focal,#//16,
                                                                          formulation= self.depth_reciprocal_formulation,
                                                                          geometry_method= self.depth_geometry_method,
                                                                          project_operator= self.depth_project_operator,
                                                                          full_homo= self.depth_full_homo,
                                                                          )
                if "layer4" in self.depth_reciprocal_pos or "all" in self.depth_reciprocal_pos:
                    self.backbone.layer4[1].conv2 = DepthReciprocalConv2d(conv= self.backbone.layer4[1].conv2,
                                                                          pass_warped_images= self.depth_reciprocal_warp,
                                                                          style= self.depth_reciprocal_style,
                                                                          Dmin= self.depth_reciprocal_Dmin,
                                                                          Dmax= self.depth_reciprocal_Dmax,
                                                                          focal= self.depth_reciprocal_focal,#//8,
                                                                          formulation= self.depth_reciprocal_formulation,
                                                                          geometry_method= self.depth_geometry_method,
                                                                          project_operator= self.depth_project_operator,
                                                                          full_homo= self.depth_full_homo,
                                                                          )
                if "layer3" in self.depth_reciprocal_pos or "all" in self.depth_reciprocal_pos:
                    # Layer 3
                    self.backbone.layer3[1].conv1 = DepthReciprocalConv2d(conv= self.backbone.layer3[1].conv1,
                                                                          pass_warped_images= self.depth_reciprocal_warp,
                                                                          style= self.depth_reciprocal_style,
                                                                          Dmin= self.depth_reciprocal_Dmin,
                                                                          Dmax= self.depth_reciprocal_Dmax,
                                                                          focal= self.depth_reciprocal_focal,#//8,
                                                                          formulation= self.depth_reciprocal_formulation,
                                                                          geometry_method= self.depth_geometry_method,
                                                                          project_operator= self.depth_project_operator,
                                                                          full_homo= self.depth_full_homo,
                                                                          )
                    self.backbone.layer3[1].conv2 = DepthReciprocalConv2d(conv= self.backbone.layer3[1].conv2,
                                                                          pass_warped_images= self.depth_reciprocal_warp,
                                                                          style= self.depth_reciprocal_style,
                                                                          Dmin= self.depth_reciprocal_Dmin,
                                                                          Dmax= self.depth_reciprocal_Dmax,
                                                                          focal= self.depth_reciprocal_focal,#//8,
                                                                          formulation= self.depth_reciprocal_formulation,
                                                                          geometry_method= self.depth_geometry_method,
                                                                          project_operator= self.depth_project_operator,
                                                                          full_homo= self.depth_full_homo,
                                                                          )
            else:
                raise NotImplementedError

        elif self.use_conv in ["coord", "deformable", "lie", "motion_basis"]:
            output_string   = "Replace {} layer by {} conv... ".format(self.lie_pos, self.use_conv)
            logging.info(output_string)
            if self.coord_conv_style in ["directions", "plucker", "directions_plucker"] and self.use_crop_for_plucker:
                logging.info("Using crops for rays with {} v{}...".format(self.coord_conv_style, self.plucker_version))
            if "first" in self.lie_pos or "all" in self.lie_pos:
                self.backbone.base_layer[0]  = LieDeformableConv2D(conv= self.backbone.base_layer[0], style= self.use_conv,
                                                  gd_homo= self.gd_homo, init_offset= self.init_offset,
                                                  coord_conv= self.coord_conv,
                                                  offset_append_coord= self.offset_append_coord,
                                                  explicit_convolve_coord= self.explicit_convolve_coord,
                                                  use_normal= self.use_normal,
                                                  coord_conv_style= self.coord_conv_style,
                                                  coord_conv_merging= self.coord_conv_merging,
                                                  coord_fourier_scale= self.coord_fourier_scale,
                                                  coord_fourier_channels= self.coord_fourier_channels
                                                  )
            if "last" in self.lie_pos or "all" in self.lie_pos:
                self.backbone.level5.project[0]   = LieDeformableConv2D(conv= self.backbone.level5.project[0], style= self.use_conv,
                                                  gd_homo= self.gd_homo, init_offset= self.init_offset,
                                                  coord_conv= self.coord_conv,
                                                  offset_append_coord= self.offset_append_coord,
                                                  explicit_convolve_coord= self.explicit_convolve_coord,
                                                  use_normal= self.use_normal,
                                                  coord_conv_style= self.coord_conv_style,
                                                  coord_conv_merging= self.coord_conv_merging,
                                                  coord_fourier_scale= self.coord_fourier_scale,
                                                  coord_fourier_channels= self.coord_fourier_channels
                                                  )
            if "all" in self.lie_pos:
                self.backbone.level0[0] = LieDeformableConv2D(conv= self.backbone.level0[0], style= self.use_conv,
                                                  gd_homo= self.gd_homo, init_offset= self.init_offset,
                                                  coord_conv= self.coord_conv,
                                                  offset_append_coord= self.offset_append_coord,
                                                  explicit_convolve_coord= self.explicit_convolve_coord,
                                                  use_normal= self.use_normal,
                                                  coord_conv_style= self.coord_conv_style,
                                                  coord_conv_merging= self.coord_conv_merging,
                                                  coord_fourier_scale= self.coord_fourier_scale,
                                                  coord_fourier_channels= self.coord_fourier_channels
                                                  )
                self.backbone.level1[0] = LieDeformableConv2D(conv= self.backbone.level1[0], style= self.use_conv,
                                                  gd_homo= self.gd_homo, init_offset= self.init_offset,
                                                  coord_conv= self.coord_conv,
                                                  offset_append_coord= self.offset_append_coord,
                                                  explicit_convolve_coord= self.explicit_convolve_coord,
                                                  use_normal= self.use_normal,
                                                  coord_conv_style= self.coord_conv_style,
                                                  coord_conv_merging= self.coord_conv_merging,
                                                  coord_fourier_scale= self.coord_fourier_scale,
                                                  coord_fourier_channels= self.coord_fourier_channels
                                                  )
                self.backbone.level2.project[0] = LieDeformableConv2D(conv= self.backbone.level2.project[0], style= self.use_conv,
                                                  gd_homo= self.gd_homo, init_offset= self.init_offset,
                                                  coord_conv= self.coord_conv,
                                                  offset_append_coord= self.offset_append_coord,
                                                  explicit_convolve_coord= self.explicit_convolve_coord,
                                                  use_normal= self.use_normal,
                                                  coord_conv_style= self.coord_conv_style,
                                                  coord_conv_merging= self.coord_conv_merging,
                                                  coord_fourier_scale= self.coord_fourier_scale,
                                                  coord_fourier_channels= self.coord_fourier_channels
                                                  )
                self.backbone.level3.project[0] = LieDeformableConv2D(conv= self.backbone.level3.project[0], style= self.use_conv,
                                                  gd_homo= self.gd_homo, init_offset= self.init_offset,
                                                  coord_conv= self.coord_conv,
                                                  offset_append_coord= self.offset_append_coord,
                                                  explicit_convolve_coord= self.explicit_convolve_coord,
                                                  use_normal= self.use_normal,
                                                  coord_conv_style= self.coord_conv_style,
                                                  coord_conv_merging= self.coord_conv_merging,
                                                  coord_fourier_scale= self.coord_fourier_scale,
                                                  coord_fourier_channels= self.coord_fourier_channels
                                                  )
                self.backbone.level4.project[0] = LieDeformableConv2D(conv= self.backbone.level4.project[0], style= self.use_conv,
                                                  gd_homo= self.gd_homo, init_offset= self.init_offset,
                                                  coord_conv= self.coord_conv,
                                                  offset_append_coord= self.offset_append_coord,
                                                  explicit_convolve_coord= self.explicit_convolve_coord,
                                                  use_normal= self.use_normal,
                                                  coord_conv_style= self.coord_conv_style,
                                                  coord_conv_merging= self.coord_conv_merging,
                                                  coord_fourier_scale= self.coord_fourier_scale,
                                                  coord_fourier_channels= self.coord_fourier_channels
                                                  )
        if np.abs(self.depth_pp) > 0.01:
            logging.info("Postprocessing Depth with version {}".format(self.depth_pp_version))

        # initialize the head of pipeline, according to heads setting.
        self.heatmap = nn.Sequential(nn.Conv2d(channels[self.first_level], self.head_conv, kernel_size=3, padding=1, bias=True),
                                     nn.ReLU(inplace=True),
                                     nn.Conv2d(self.head_conv, self.num_classes, kernel_size=1, stride=1, padding=0, bias=True))
        self.offset_2d = nn.Sequential(nn.Conv2d(channels[self.first_level], self.head_conv, kernel_size=3, padding=1, bias=True),
                                     nn.ReLU(inplace=True),
                                     nn.Conv2d(self.head_conv, 2, kernel_size=1, stride=1, padding=0, bias=True))
        self.size_2d = nn.Sequential(nn.Conv2d(channels[self.first_level], self.head_conv, kernel_size=3, padding=1, bias=True),
                                     nn.ReLU(inplace=True),
                                     nn.Conv2d(self.head_conv, 2, kernel_size=1, stride=1, padding=0, bias=True))


        self.depth = nn.Sequential(nn.Conv2d(channels[self.first_level]+2+self.cls_num, self.head_conv, kernel_size=3, padding=1, bias=True),
                                     nn.BatchNorm2d(self.head_conv),
                                     nn.ReLU(inplace=True),nn.AdaptiveAvgPool2d(1),
                                     nn.Conv2d(self.head_conv, 2, kernel_size=1, stride=1, padding=0, bias=True))
        self.offset_3d = nn.Sequential(nn.Conv2d(channels[self.first_level]+2+self.cls_num, self.head_conv, kernel_size=3, padding=1, bias=True),
                                     nn.BatchNorm2d(self.head_conv),
                                     nn.ReLU(inplace=True),nn.AdaptiveAvgPool2d(1),
                                     nn.Conv2d(self.head_conv, 2, kernel_size=1, stride=1, padding=0, bias=True))
        self.size_3d = nn.Sequential(nn.Conv2d(channels[self.first_level]+2+self.cls_num, self.head_conv, kernel_size=3, padding=1, bias=True),
                                     nn.BatchNorm2d(self.head_conv),
                                     nn.ReLU(inplace=True),nn.AdaptiveAvgPool2d(1),
                                     nn.Conv2d(self.head_conv, 4, kernel_size=1, stride=1, padding=0, bias=True))
        self.heading = nn.Sequential(nn.Conv2d(channels[self.first_level]+2+self.cls_num, self.head_conv, kernel_size=3, padding=1, bias=True),
                                     nn.BatchNorm2d(self.head_conv),
                                     nn.ReLU(inplace=True),nn.AdaptiveAvgPool2d(1),
                                     nn.Conv2d(self.head_conv, 24, kernel_size=1, stride=1, padding=0, bias=True))

        if self.use_ground_plane:
            self.alpha = nn.Sequential(nn.Conv2d(channels[self.first_level]+2+self.cls_num, self.head_conv, kernel_size=3, padding=1, bias=True),
                             nn.BatchNorm2d(self.head_conv),
                             nn.ReLU(inplace=True),nn.AdaptiveAvgPool2d(1),
                             nn.Conv2d(self.head_conv, 2, kernel_size=1, stride=1, padding=0, bias=True))

        if self.ground_merging_learnt:
            self.learnt = nn.Sequential(nn.Conv2d(channels[self.first_level]+2+self.cls_num, self.head_conv, kernel_size=3, padding=1, bias=True),
                 nn.BatchNorm2d(self.head_conv),
                 nn.ReLU(inplace=True),nn.AdaptiveAvgPool2d(1),
                 nn.Conv2d(self.head_conv, 1, kernel_size=1, stride=1, padding=0, bias=True),
                 nn.Sigmoid())

        # init layers
        self.heatmap[-1].bias.data.fill_(-2.19)
        self.fill_fc_weights(self.offset_2d)
        self.fill_fc_weights(self.size_2d)

        self.depth.apply(weights_init_xavier)
        self.offset_3d.apply(weights_init_xavier)
        self.size_3d.apply(weights_init_xavier)
        self.heading.apply(weights_init_xavier)

    def forward(self, input, coord_ranges,calibs, targets=None, info=None, K=50, mode='train'):
        device_id = input.device
        BATCH_SIZE = input.size(0)

        if "oracle" in self.coord_conv_style or "directions" in self.coord_conv_style or "plucker" in self.coord_conv_style:
            if "depth" in self.coord_conv_style:
                assert "depth_map" in targets.keys()
                depth_map = targets["depth_map"]
                mask      = depth_map < 1
                inv_depth_map = 1.0/(depth_map + 1e-2)
                inv_depth_map[mask] = 1
            if "normal" in self.coord_conv_style:
                assert "normal" in targets.keys()
                normal_map = targets["normal"]
            if "kprpe" in self.coord_conv_style:
                center_2d = targets["center_2d"] # B x 50 x 2
                size_2d   = targets["size_2d"]   # B x 50 x 2
                w2d       = targets["size_2d"][:, :, 0].unsqueeze(2)
                h2d       = targets["size_2d"][:, :, 1].unsqueeze(2)

                top_left     = center_2d - 0.5*size_2d
                top_right    = center_2d + torch.cat([0.5*w2d, - 0.5*h2d], dim= 2)
                bottom_left  = center_2d + torch.cat([-0.5*w2d,  0.5*h2d], dim= 2)
                bottom_right = center_2d + 0.5*size_2d
                keypoints    = torch.cat([center_2d.unsqueeze(1), top_left.unsqueeze(1), top_right.unsqueeze(1), bottom_left.unsqueeze(1), bottom_right.unsqueeze(1)], dim= 1) # B x 5 x 50 x 2
                keypoints    = keypoints.reshape(BATCH_SIZE, -1, 2)

            if "directions" in self.coord_conv_style or "plucker" in self.coord_conv_style:
                # compute camera matrices, intrinsics and finally rays
                ones    = torch.zeros_like(calibs[:, 0].unsqueeze(1)).type(input.dtype).to(input.device)
                ones[:, :, 3] = 1.0


                ref_ext = torch.eye(4).type(input.dtype).to(input.device).unsqueeze(0).repeat(BATCH_SIZE, 1, 1) # B x 4 x 4
                ref_ext[:, 1, 3] = 1.519076                                            # B x 4 x 4
                intrinsics = info["intrinsics"].type(input.dtype).to(input.device)     # B x 3 x 3
                intrinsics_4 = torch.zeros_like(ref_ext).type(input.dtype).to(input.device)
                intrinsics_4[:, :3, :3] = intrinsics
                intrinsics_4[:,  3,  3] = 1.0

                gd_to_cam = info["gd_to_cam"].type(input.dtype).to(input.device)        # B x 3 x 4
                gd_to_cam = torch.concat((gd_to_cam, ones), dim=1)                     # B x 4 x 4

                if self.plucker_version == 1:
                    cameras   = intrinsics_4 @ gd_to_cam @ torch.linalg.inv(ref_ext)       # B x 4 x 4

                    intrinsics = get_intrinsics_from_fov(h= 512, w= 512, fov_degree=64.56, four_by_four= True)
                    intrinsics = torch.from_numpy(intrinsics).to(input.device).type(input.dtype).unsqueeze(0).repeat(BATCH_SIZE, 1, 1) # B x 4 x 4
                    if self.use_crop_for_plucker:
                        crop_parameters = trans_to_crop_params(info['trans'], intrinsics[:, :3, :3], im_width_height= info['img_size'][0])
                    else:
                        crop_parameters =  None
                    rays       = cameras_to_rays(cameras, intrinsics[:, :3, :3], crop_parameters= crop_parameters, return_mode= self.coord_conv_style)
                else:
                    trans           = torch.zeros_like(intrinsics)      # B x 3 x 3
                    trans[:, :2]    = info['trans']
                    trans[:,  2, 2] = 1.0
                    if self.use_crop_for_plucker:
                        intrinsics   = trans @ intrinsics               # B x 3 x 3
                    # c2w for camera at 2.25m truck height with reference camera at 1.5m
                    #  --             --
                    # | 1  0  0  0     |
                    # | 0  1  0  -0.75 |
                    # | 0  0  1  0     |
                    # | 0  0  0  0     |
                    # --             --
                    c2w        = torch.linalg.inv(gd_to_cam @ torch.linalg.inv(ref_ext)) # B x 4 x 4

                    rays       = plucker_embedding(H= info['img_size'][0][1].item(), W= info['img_size'][0][0].item(), intrinsics= intrinsics, c2w= c2w, jitter=False)


            if "canny" in self.coord_conv_style:
                assert "canny" in targets.keys()
                canny = targets["canny"].float()

            if self.coord_conv_style == "oracle_depth":
                extra = depth_map
            elif self.coord_conv_style == "oracle_inv_depth":
                extra = inv_depth_map
            elif self.coord_conv_style == "oracle_normal":
                extra = normal_map
            elif self.coord_conv_style == "oracle_normal_inv_depth":
                extra = normal_map * inv_depth_map.repeat(1, 3, 1, 1)
            elif self.coord_conv_style == "oracle_kprpe":
                extra = keypoints
            elif self.coord_conv_style == "oracle_canny":
                extra = canny
            elif self.coord_conv_style == "oracle_normal_canny":
                extra = normal_map * canny.repeat(1, 3, 1, 1)
            elif self.coord_conv_style == "oracle_normal_canny_inv":
                extra = normal_map * (1.0 - canny.repeat(1, 3, 1, 1))
            elif "directions" in self.coord_conv_style or "plucker" in self.coord_conv_style:
                extra = rays
            else:
                raise NotImplementedError
            input = (input, extra)

        feat = self.backbone(input)
        if isinstance(feat, list):
            if self.use_conv == "lie":
                feat, lie_wt = feat
            if self.use_conv not in ["lie"]:
                lie_wt = None
                feat = feat[0] if len(feat) == 2 else feat  # Returning some extra
        else:
            lie_wt = None
        if self.get_backbone_features:
            return [lie_wt]
        feat = self.feat_up(feat[self.first_level:])
        '''
        ret = {}
        for head in self.heads:
            ret[head] = self.__getattr__(head)(feat)
        '''
        ret = {}
        ret['heatmap']=self.heatmap(feat)
        ret['offset_2d']=self.offset_2d(feat)
        ret['size_2d']=self.size_2d(feat)
        #two stage
        assert(mode in ['train','val','test'])
        if mode=='train':   #extract train structure in the train (only) and the val mode
            inds,cls_ids = targets['indices'],targets['cls_ids']
            masks = targets['mask_2d'].bool()
        else:    #extract test structure in the test (only) and the val mode
            inds,cls_ids = _topk(_nms(torch.clamp(ret['heatmap'].sigmoid(), min=1e-4, max=1 - 1e-4)), K=K)[1:3]
            masks = torch.ones(inds.size()).type(torch.uint8).to(device_id).bool()
        ret.update(self.get_roi_feat(feat,inds,masks,ret,calibs,coord_ranges,cls_ids,info))
        ret['lie_wt'] = lie_wt
        return ret

    def get_roi_feat_by_mask(self,feat,box2d_maps,inds,mask,calibs,coord_ranges,cls_ids,info):
        BATCH_SIZE,_,HEIGHT,WIDE = feat.size()
        device_id = feat.device
        num_masked_bin = mask.sum()
        res = {}
        if num_masked_bin!=0:
            #get box2d of each roi region
            box2d_masked = extract_input_from_tensor(box2d_maps,inds,mask)
            #get roi feature
            roi_feature_masked = roi_align(feat,box2d_masked,[7,7])
            #get coord range of each roi
            coord_ranges_mask2d = coord_ranges[box2d_masked[:,0].long()]

            #map box2d coordinate from feature map size domain to original image size domain
            box2d_masked = torch.cat([box2d_masked[:,0:1],
                       box2d_masked[:,1:2]/WIDE  *(coord_ranges_mask2d[:,1,0:1]-coord_ranges_mask2d[:,0,0:1])+coord_ranges_mask2d[:,0,0:1],
                       box2d_masked[:,2:3]/HEIGHT*(coord_ranges_mask2d[:,1,1:2]-coord_ranges_mask2d[:,0,1:2])+coord_ranges_mask2d[:,0,1:2],
                       box2d_masked[:,3:4]/WIDE  *(coord_ranges_mask2d[:,1,0:1]-coord_ranges_mask2d[:,0,0:1])+coord_ranges_mask2d[:,0,0:1],
                       box2d_masked[:,4:5]/HEIGHT*(coord_ranges_mask2d[:,1,1:2]-coord_ranges_mask2d[:,0,1:2])+coord_ranges_mask2d[:,0,1:2]],1)
            roi_calibs = calibs[box2d_masked[:,0].long()]
            #project the coordinate in the normal image to the camera coord by calibs
            coords_in_camera_coord = torch.cat([self.project2rect(roi_calibs,torch.cat([box2d_masked[:,1:3],torch.ones([num_masked_bin,1]).to(device_id)],-1))[:,:2],
                                          self.project2rect(roi_calibs,torch.cat([box2d_masked[:,3:5],torch.ones([num_masked_bin,1]).to(device_id)],-1))[:,:2]],-1)
            coords_in_camera_coord = torch.cat([box2d_masked[:,0:1],coords_in_camera_coord],-1)
            #generate coord maps
            coord_maps = torch.cat([torch.cat([coords_in_camera_coord[:,1:2]+i*(coords_in_camera_coord[:,3:4]-coords_in_camera_coord[:,1:2])/6 for i in range(7)],-1).unsqueeze(1).repeat([1,7,1]).unsqueeze(1),
                                torch.cat([coords_in_camera_coord[:,2:3]+i*(coords_in_camera_coord[:,4:5]-coords_in_camera_coord[:,2:3])/6 for i in range(7)],-1).unsqueeze(2).repeat([1,1,7]).unsqueeze(1)],1)

            #concatenate coord maps with feature maps in the channel dim
            cls_hots = torch.zeros(num_masked_bin,self.cls_num).to(device_id)
            cls_hots[torch.arange(num_masked_bin).to(device_id),cls_ids[mask].long()] = 1.0
            
            roi_feature_masked = torch.cat([roi_feature_masked,coord_maps,cls_hots.unsqueeze(-1).unsqueeze(-1).repeat([1,1,7,7])],1)
 
            #compute heights of projected objects
            box2d_height = torch.clamp(box2d_masked[:,4]-box2d_masked[:,2],min=1.0)
            #compute real 3d height
            size3d_offset = self.size_3d(roi_feature_masked)[:,:,0,0]
            h3d_log_std = size3d_offset[:,3:4]
            size3d_offset = size3d_offset[:,:3] 

            size_3d = (self.mean_size[cls_ids[mask].long()]+size3d_offset)
            # Depth with size3d
            depth_geo = size_3d[:,0]/box2d_height.squeeze()*roi_calibs[:,0,0]   # m
            depth_geo_log_std = (h3d_log_std.squeeze()+2*(roi_calibs[:,0,0].log()-box2d_height.log())).unsqueeze(-1)  # m x 1

            res['train_tag'] = torch.ones(num_masked_bin).type(torch.bool).to(device_id)
            res['heading'] = self.heading(roi_feature_masked)[:,:,0,0]
            res['offset_3d'] = self.offset_3d(roi_feature_masked)[:,:,0,0]
            res['size_3d']= size3d_offset
            res['h3d_log_variance'] = h3d_log_std

            if self.use_ground_plane:
                downsample = 4
                # 2D box centers
                v = 0.5*(box2d_masked[:,2] + box2d_masked[:,4])

                # Projected 3D box centers
                coord_map    = torch.cat([torch.arange(WIDE).unsqueeze(0).repeat([HEIGHT,1]).unsqueeze(0),\
                                torch.arange(HEIGHT).unsqueeze(-1).repeat([1,WIDE]).unsqueeze(0)],0).unsqueeze(0).repeat([BATCH_SIZE,1,1,1]).type(torch.float).to(device_id)
                box2d_masked = extract_input_from_tensor(box2d_maps, inds, mask)  # M x 5  index, x1, y1, x2, y2
                c            = box2d_masked[:, 1:3] + res['offset_3d']
                cx, cy       = c[:, 0], c[:, 1]

                # Simple number
                alpha_alpha_unc = self.alpha(roi_feature_masked)[:, :, 0, 0]  # m x 2
                alpha     = alpha_alpha_unc[:, 0]                             # m
                alpha_unc = alpha_alpha_unc[:, 1].unsqueeze(1)                # m x 1

                if self.ground_merging_learnt:
                    learnt_val = self.learnt(roi_feature_masked)[:, :, 0, 0]  # m x 1
                    learnt_val = torch.clamp(learnt_val.clone(), 0.01, 0.99)

                # Get the bottom center.
                bx    = cx
                if self.ground_alpha == 'product':
                    by    = cy + 0.5*box2d_height + alpha*(cy - v)     # m
                elif self.ground_alpha == 'sum':
                    if self.ground_alpha_max > 0:
                        alpha = torch.clamp(alpha, -self.ground_alpha_max, self.ground_alpha_max)
                    by    = cy + 0.5*box2d_height + alpha              # m
                else:
                    by    = cy + 0.5*box2d_height                      # m
                bx    = torch.clamp(bx, 0.0, WIDE   - 1.0)
                by    = torch.clamp(by, 0.0, HEIGHT - 1.0)
                U     = bx.unsqueeze(0).unsqueeze(0).cuda() # 1 x 1 x m
                V     = by.unsqueeze(0).unsqueeze(0).cuda() # 1 x 1 x m

                # Camera params
                img_size   = info["img_size"]                    # B x 2
                intrinsics = info["intrinsics"].type(feat.dtype).to(device_id) # B x 3 x 3
                trans      = info["trans"].type(feat.dtype).to(device_id)      # B x 2 x 3
                ones       = torch.from_numpy(np.array( [[0, 0., 1.]]) ).unsqueeze(0).repeat(BATCH_SIZE, 1, 1).type(feat.dtype).to(device_id) # B x 1 x 3
                trans      = torch.concat((trans, ones), dim= 1) # B x 3 x 3
                extrinsics = torch.eye(4).unsqueeze(0).repeat(BATCH_SIZE, 1, 1).type(feat.dtype).to(device_id)                                # B x 4 x 4
                cam_height = info["gd_to_cam"][:, 1, 3].to(device_id)  # B

                # Select indices
                img_index  = box2d_masked[:, 0].long()

                depth_center = ground_depth  (intrinsics= intrinsics[img_index],
                                              extrinsics= extrinsics[img_index],
                                              cam_height= cam_height[img_index],
                                              im_h= HEIGHT*downsample,
                                              im_w= WIDE*downsample,
                                              downsample= downsample,
                                              trans= trans[img_index],
                                              U= U, V= V)[0, 0]                   # m

                if self.ground_depth_positive:
                    pos_mask = depth_center.clone().detach() > 0
                    depth_inifinity = 100* torch.ones_like(depth_center.clone())
                    depth_center = pos_mask * depth_center + torch.logical_not(pos_mask) * depth_inifinity


                # Convert to respective formulations
                if self.ground_formulation == 'depth':
                    depth_net_out  = torch.cat([depth_center.unsqueeze(1), alpha_unc], dim= 1)                       # m x 2
                elif self.ground_formulation == 'depth_relu':
                    depth_net_out  = torch.cat([F.relu(depth_center.unsqueeze(1)), alpha_unc], dim= 1)               # m x 2
                elif self.ground_formulation == 'disparity':
                    depth_net_out  = torch.cat([1.0/depth_center.unsqueeze(1)          , alpha_unc], dim= 1)         # m x 2
                elif self.ground_formulation == 'disparity_relu':
                    depth_net_out  = torch.cat([F.relu(1.0/depth_center.unsqueeze(1))          , alpha_unc], dim= 1) # m x 2
                elif self.ground_formulation == 'disparity_sigmoid':
                    depth_net_out  = torch.cat([      (1.0/depth_center.unsqueeze(1)).sigmoid(), alpha_unc], dim= 1) # m x 2
                else:
                    raise NotImplementedError

                if self.ground_depth_regress or self.ground_geo_depth_twice or self.ground_geo_depth_half:
                    depth_regress  = self.depth(roi_feature_masked)[:,:,0,0]           # m x 2

                # Add uncertainties
                if self.ground_depth_regress:
                    depth_net_log_std = torch.logsumexp(torch.cat([alpha_unc,depth_regress[:,1:2], depth_geo_log_std],-1),-1,keepdim=True)          # m x 1
                elif self.ground_geo_depth_half:
                    if self.ground_merging_learnt:
                        depth_net_log_std = torch.logsumexp(torch.cat([learnt_val *alpha_unc, (1-learnt_val) *depth_regress[:, 1:2], depth_geo_log_std],-1),-1, keepdim=True) # m x 1
                    else:
                        depth_net_log_std = torch.logsumexp(torch.cat([0.5*alpha_unc, 0.5*depth_regress[:, 1:2], depth_geo_log_std],-1),-1, keepdim=True) # m x 1
                elif self.ground_geo_depth_twice:
                    depth_net_log_std = torch.logsumexp(torch.cat([alpha_unc, depth_regress[:, 1:2], depth_geo_log_std, depth_geo_log_std],-1),-1, keepdim=True) # m x 1
                else:
                    depth_net_log_std = torch.logsumexp(torch.cat([alpha_unc, depth_geo_log_std],-1),-1,keepdim=True)          # m x 1

                # Convert formulations to depth now
                if self.ground_formulation in ['depth', 'depth_relu']:
                    t = (depth_net_out[:,0:1] + 1e-6)
                elif self.ground_formulation in ['disparity', 'disparity_relu', 'disparity_sigmoid']:
                    t = (1. / (depth_net_out[:,0:1] + 1e-6) - 1.)
                else:
                    raise NotImplementedError

                # Get final depths
                if self.ground_depth_regress:
                    depth_net_out     = torch.cat([t + (1. / (depth_regress[:,0:1].sigmoid() + 1e-6) - 1.) + depth_geo.unsqueeze(-1), depth_net_log_std],-1)   # m x 2
                elif self.ground_geo_depth_twice:
                    depth_net_out     = torch.cat([t + (1. / (depth_regress[:,0:1].sigmoid() + 1e-6) - 1.) + 2*depth_geo.unsqueeze(-1), depth_net_log_std],-1)   # m x 2
                elif self.ground_geo_depth_half:
                    if self.ground_merging_learnt:
                        depth_net_out     = torch.cat([learnt_val * t + (1- learnt_val) * (1. / (depth_regress[:,0:1].sigmoid() + 1e-6) - 1.) + depth_geo.unsqueeze(-1), depth_net_log_std],-1)   # m x 2
                    else:
                        depth_net_out     = torch.cat([0.5 * t + 0.5 * (1. / (depth_regress[:,0:1].sigmoid() + 1e-6) - 1.) + depth_geo.unsqueeze(-1), depth_net_log_std],-1)   # m x 2
                else:
                    depth_net_out     = torch.cat([t + depth_geo.unsqueeze(-1), depth_net_log_std],-1)   # m x 2


            else:
                # Disparity and sigma from depth regression
                depth_net_out = self.depth(roi_feature_masked)[:,:,0,0]           # m x 2
                # Add uncertainties
                depth_net_log_std = torch.logsumexp(torch.cat([depth_net_out[:,1:2],depth_geo_log_std],-1),-1,keepdim=True)

                depth_net_out = torch.cat([(1. / (depth_net_out[:,0:1].sigmoid() + 1e-6) - 1.)+depth_geo.unsqueeze(-1),depth_net_log_std],-1)

            res['depth']   = postprocess_depth(depth_net_out, depth_pp= self.depth_pp, version= self.depth_pp_version)

        else:
            res['depth'] = torch.zeros([1,2]).to(device_id)
            res['offset_3d'] = torch.zeros([1,2]).to(device_id)
            res['size_3d'] = torch.zeros([1,3]).to(device_id)
            res['train_tag'] = torch.zeros(1).type(torch.bool).to(device_id)
            res['heading'] = torch.zeros([1,24]).to(device_id)
            res['h3d_log_variance'] = torch.zeros([1,1]).to(device_id)
        return res

    def get_roi_feat(self,feat,inds,mask,ret,calibs,coord_ranges,cls_ids,info):
        BATCH_SIZE,_,HEIGHT,WIDE = feat.size()
        device_id = feat.device
        coord_map = torch.cat([torch.arange(WIDE).unsqueeze(0).repeat([HEIGHT,1]).unsqueeze(0),\
                        torch.arange(HEIGHT).unsqueeze(-1).repeat([1,WIDE]).unsqueeze(0)],0).unsqueeze(0).repeat([BATCH_SIZE,1,1,1]).type(torch.float).to(device_id)
        box2d_centre = coord_map + ret['offset_2d']
        box2d_maps = torch.cat([box2d_centre-ret['size_2d']/2,box2d_centre+ret['size_2d']/2],1)
        box2d_maps = torch.cat([torch.arange(BATCH_SIZE).unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).repeat([1,1,HEIGHT,WIDE]).type(torch.float).to(device_id),box2d_maps],1)
        #box2d_maps is box2d in each bin
        res = self.get_roi_feat_by_mask(feat,box2d_maps,inds,mask,calibs,coord_ranges,cls_ids,info)
        return res


    def project2rect(self,calib,point_img):
        c_u = calib[:,0,2]
        c_v = calib[:,1,2]
        f_u = calib[:,0,0]
        f_v = calib[:,1,1]
        b_x = calib[:,0,3]/(-f_u) # relative
        b_y = calib[:,1,3]/(-f_v)
        x = (point_img[:,0]-c_u)*point_img[:,2]/f_u + b_x
        y = (point_img[:,1]-c_v)*point_img[:,2]/f_v + b_y
        z = point_img[:,2]
        centre_by_obj = torch.cat([x.unsqueeze(-1),y.unsqueeze(-1),z.unsqueeze(-1)],-1)
        return centre_by_obj

    def fill_fc_weights(self, layers):
        for m in layers.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.normal_(m.weight, std=0.001)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def adapt_in_eval(self, shift_coord_conv, eval_gd_homo = None):
        if shift_coord_conv is not None or self.eval_gd_homo is not None:
            logging.info("Adapting model...")
            self.shift_coord_conv = torch.tensor(shift_coord_conv) if shift_coord_conv is not None else None
            self.eval_gd_homo     = torch.tensor(eval_gd_homo)     if eval_gd_homo     is not None else None
            adapt_model(self, self.shift_coord_conv, self.eval_gd_homo)

    def reset_adapt_in_eval(self):
        if self.shift_coord_conv is not None or self.eval_gd_homo is not None:
            logging.info("Reseting model...")
            reset_adapt_model(self)


if __name__ == '__main__':
    import torch
    net = CenterNet3D()
    print(net)
    input = torch.randn(4, 3, 384, 1280)
    print(input.shape, input.dtype)
    output = net(input)
